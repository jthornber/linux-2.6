* Introduction

Multisnap is refers to a collection of device-mapper targets that
between them implement thin provisioning and snapshots.

The main highlight of this implementation, compared to the previous
implementation of snapshots, is it allows many virtual devices to be
stored on the same data volume.  Simplifying administration and
allowing sharing of data between volumes (thus reducing disk usage).

Another big feature is support for arbitrary depth of recursive
snapshots (snapshots of snapshots of snapshots ...).  Previous
implementations of snapshots did this by chaining together lookup
tables, and so performance was O(depth).  This implementation uses a
single table so we wont get this degradation with depth (fragmentation
may be an issue however in some scenarios).

Metadata is stored on a separate device from data, this gives the
administrator a bit more freedom.  For instance:

- Improve metadata resilience by storing metadata on a mirrored volume
  but data on a non-mirrored one.

- Improve performance by storing the metadata on an SSD.

* Status

These targets are very much in the EXPERIMENTAL state.  Do not use in
production.

_Do_ experiment with it and give us feedback.  Different use cases
will have different performance characteristics (for example due to
fragmentation of the data volume).  If you find this software is
performing as expected please mail dm-devel@redhat.com with details
and we'll try our best to improve things for you.

* Cookbook

This section describes some quick recipes for using multisnap using
the dmsetup program to control the device-mapper driver directly.  End
users are advised to use a higher level volume manager such as LVM2.

** Pool device

The pool device ties together the metadata volume and the data volume.
It linearly maps the data volume, and updates the metadata via two
mechanisms:

- Function calls from the multisnap targets

- device-mapper 'messages' from userland which control creation of new
  virtual devices among other things.

** Setting up a fresh pool device

Setting up a pool device requires a _valid_ metadata device, and a
data device.  If you do not have an existing metadata device you can
make one by zeroing the first 4k to indicate empty metadata.

    dd if=/dev/zero of=$metadata_dev bs=4096 count=1

** Using an existing pool device

The pool device must have the same size as the data device, since it
reflects this via a linear mapping.

    dmsetup create pool --table "0 20971520 multisnap-pool $metadata_dev $data_dev $data_block_size $low_water_mark"

The $data_block_size gives the smallest unit of disk space that can be
allocated at a time.  This is expressed in 512 byte sectors.  People
primarily interested in thin provisioning may want to set this larger
(e.g., 1024).  People doing lots of snapshotting may want it smaller
(e.g., 128).  $data_block_size must be the same for the lifetime of
the metadata device.

The $low_water_mark is expressed in units of $data_block_size, if free
space on the data device drops below this level then a dm event will
be sent to userland.  This event is edge triggered, it will occur only
once, so volume manager writers should register for the event, then
double check the status of the target.  FIXME: move last sentence to
reference.

** Thin provisioning

*** Creating a new thinly provisioned volume

To create a new thin provision volume you must send a message to an
active pool device.

    dmsetup message /dev/mapper/pool 0 "new-thin 0 2097152"

Here '0' is an identifier for the volume (32bit range).  It's up to
the caller to allocate and manager these identifiers.  If there's a
collision the message will fail.

'2097152' is the size of the virtual device.


*** Using a thinp volume

Thin provisioned volumes are instanced using the 'multisnap' target.

    dmsetup create thin --table "0 2097152 multisnap /dev/mapper/pool $metadata_dev 0"

We need to reference both the pool device and the metadata device
(this is ugly).  Finally the last parameter is the 32bit identifier
for the thinp device.

** Internal snapshots

*** Creating an internal snapshot

Snapshots are created with another message to the pool.

You _must_ suspend the origin device before creating the snapshot.  If
the origin hasn't been instanced via the 'multisnap' target then you
may proceed without doing anything.

    dmsetup suspend /dev/mapper/thin
    dmsetup message /dev/mapper/pool 0 "new-snap 1 0"
    dmsetup resume /dev/mapper/thin

*** Using an internal snapshot

Once created, the user doesn't have to worry about any connection
between the origin and the snapshot.  Indeed the snapshot is no
different from any other thinly provisioned device, and can be
snapshotted itself via the same method.  It's perfectly legal to have
only one them active, and there's no ordering requirement on
activating/removing them both.

Activate exactly the same way as any other thin provisioned volume.

    dmsetup create snap --table "0 2097152 multisnap /dev/mapper/pool $metadata_dev 1"

** Teardown

Always teardown the pool last.

    dmsetup remove thin
    dmsetup remove snap
    dmsetup remove pool

* Reference

** multisnap-pool target

*** Constructor

 multisnap-pool <metadata dev>
                <data dev>
                <data block size in sectors>
                <low water mark (sectors)>

** Messages

 new-thin <dev id> <dev size in sectors>
 new-snap <dev id> <origin id>
 del      <dev id>

** multisnap target

 multisnap <pool dev> <metadata dev> <dev id>

 pool dev: the path to the pool (eg, /dev/mapper/my_pool)
 dev id: the internal device identifier (32bit value)








